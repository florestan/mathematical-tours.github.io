\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}}
\newlabel{eq-general-pbm}{{1}{2}{Unconstraint optimization}{equation.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Left: linear regression, middle: linear classifier, right: loss function for classification. }}{3}{figure.1}}
\newlabel{fig-ml-ex}{{1}{3}{Left: linear regression, middle: linear classifier, right: loss function for classification}{figure.1}{}}
\newlabel{eq-general-pbm-min}{{2}{3}{Unconstraint optimization}{equation.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}}
\newlabel{eq-least-square}{{3}{3}{Regression}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}}
\newlabel{eq-classif}{{4}{3}{Classification}{equation.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness. }}{4}{figure.2}}
\newlabel{fig-minimizer-exists}{{2}{4}{Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness}{figure.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Coercivity condition for least squares. }}{4}{figure.3}}
\newlabel{fig-least-square}{{3}{4}{Coercivity condition for least squares}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}}
\newlabel{eq-convexity-def}{{5}{4}{Convexity}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions. }}{5}{figure.4}}
\newlabel{fig-cvx-vs-noncvx}{{4}{5}{Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of convex functions $f : \mathbb  {R}^p \rightarrow \mathbb  {R}$ (for $p=1$) and convex sets $C \subset \mathbb  {R}^p$ (for $p=2$). }}{5}{figure.5}}
\newlabel{fig-cvx-set}{{5}{5}{Comparison of convex functions $f : \RR ^p \rightarrow \RR $ (for $p=1$) and convex sets $C \subset \RR ^p$ (for $p=2$)}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Strict convexity.}{5}{section*.2}}
\newlabel{eq-strict-convexity-def}{{6}{5}{Strict convexity}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivative and gradient}{6}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient}{6}{subsection.3.1}}
\newlabel{eq-grad-dfn}{{7}{6}{Gradient}{equation.3.7}{}}
\newlabel{prop-above-tgt}{{1}{6}{}{prop.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Function with local maxima/minima (left), saddle point (middle) and global minimum (right). }}{7}{figure.6}}
\newlabel{fig-first-order}{{6}{7}{Function with local maxima/minima (left), saddle point (middle) and global minimum (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}First Order Conditions}{7}{subsection.3.2}}
\newlabel{prop-cs-min}{{2}{7}{}{prop.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Least Squares}{8}{subsection.3.3}}
\newlabel{eq-grad-ls}{{8}{8}{Least Squares}{equation.3.8}{}}
\newlabel{eq-sol-leastsquare}{{9}{8}{Least Squares}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$. }}{9}{figure.7}}
\newlabel{fig-link-pca}{{7}{9}{Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$}{figure.7}{}}
\newlabel{eq-pca-decomp}{{10}{9}{Link with PCA}{equation.3.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chain Rule}{10}{subsection.3.6}}
\newlabel{eq-grad-composition-linear}{{11}{10}{Chain Rule}{equation.3.11}{}}
\newlabel{eq-differential-defn}{{12}{10}{Chain Rule}{equation.3.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}}
\newlabel{sec-grad-desc-basic}{{4}{10}{Gradient Descent Algorithm}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof. }}{11}{figure.8}}
\newlabel{fig-expansion-taylor}{{8}{11}{Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right). }}{12}{figure.9}}
\newlabel{fig-gradesc}{{9}{12}{Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right)}{figure.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descent}{12}{subsection.4.2}}
\newlabel{eq-grad-desc}{{13}{12}{Gradient Descent}{equation.4.13}{}}
\newlabel{eq-armijo-rule}{{14}{12}{Armijo rule}{equation.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convergence Analysis}{13}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quadratic Case}{13}{subsection.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis for the quadratic case.}{13}{section*.3}}
\newlabel{prop-graddesc-quad}{{4}{13}{}{prop.4}{}}
\newlabel{eq-global-linrate-grad}{{15}{13}{}{equation.5.15}{}}
\newlabel{eq-best-rate-local}{{16}{13}{}{equation.5.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Contraction constant $h(\tau )$ for a quadratic function (right). }}{14}{figure.10}}
\newlabel{fig-grad-desc-contract}{{10}{14}{Contraction constant $h(\tau )$ for a quadratic function (right)}{figure.10}{}}
\newlabel{eq-rate-strong-quad}{{17}{14}{Convergence analysis for the quadratic case}{equation.5.17}{}}
\newlabel{prop-graddesc-quad-sublin}{{5}{14}{}{prop.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Hessian.}{15}{section*.4}}
\newlabel{eq-taylor-hess}{{18}{15}{Hessian}{equation.5.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}}
\newlabel{eq-lipsch-grad}{{{$\mathcal  {R}_L$}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{eq-strong-conv}{{{$\mathcal  {S}_\mu $}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{prop-smooth-strong}{{6}{17}{}{prop.6}{}}
\newlabel{eq-above-below-quad}{{19}{17}{}{equation.5.19}{}}
\newlabel{eq-upper-lower-bound-hess}{{20}{17}{}{equation.5.20}{}}
\@writefile{toc}{\contentsline {paragraph