\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Motivation in Machine Learning}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Unconstraint optimization}{2}{subsection.1.1}}
\newlabel{eq-general-pbm}{{1}{2}{Unconstraint optimization}{equation.1.1}{}}
\newlabel{eq-general-pbm-min}{{2}{2}{Unconstraint optimization}{equation.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  Left: linear regression, middle: linear classifier, right: loss function for classification. }}{3}{figure.1}}
\newlabel{fig-ml-ex}{{1}{3}{Left: linear regression, middle: linear classifier, right: loss function for classification}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness. }}{3}{figure.2}}
\newlabel{fig-minimizer-exists}{{2}{3}{Left: non-existence of minimizer, middle: multiple minimizers, right: uniqueness}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Regression}{3}{subsection.1.2}}
\newlabel{eq-least-square}{{3}{3}{Regression}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Classification}{3}{subsection.1.3}}
\newlabel{eq-classif}{{4}{3}{Classification}{equation.1.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Basics of Convex Analysis}{3}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Existence of Solutions}{3}{subsection.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  Coercivity condition for least squares. }}{4}{figure.3}}
\newlabel{fig-least-square}{{3}{4}{Coercivity condition for least squares}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions. }}{4}{figure.4}}
\newlabel{fig-cvx-vs-noncvx}{{4}{4}{Convex vs. non-convex functions ; Strictly convex vs. non strictly convex functions}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Convexity}{4}{subsection.2.2}}
\newlabel{eq-convexity-def}{{5}{4}{Convexity}{equation.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Comparison of convex functions $f : \mathbb  {R}^p \rightarrow \mathbb  {R}$ (for $p=1$) and convex sets $C \subset \mathbb  {R}^p$ (for $p=2$). }}{5}{figure.5}}
\newlabel{fig-cvx-set}{{5}{5}{Comparison of convex functions $f : \RR ^p \rightarrow \RR $ (for $p=1$) and convex sets $C \subset \RR ^p$ (for $p=2$)}{figure.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Strict convexity.}{5}{section*.2}}
\newlabel{eq-strict-convexity-def}{{6}{5}{Strict convexity}{equation.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Convex Sets}{5}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Derivative and gradient}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient}{5}{subsection.3.1}}
\newlabel{eq-grad-dfn}{{7}{6}{Gradient}{equation.3.7}{}}
\newlabel{prop-above-tgt}{{1}{6}{}{prop.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}First Order Conditions}{6}{subsection.3.2}}
\newlabel{prop-cs-min}{{2}{6}{}{prop.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Function with local maxima/minima (left), saddle point (middle) and global minimum (right). }}{7}{figure.6}}
\newlabel{fig-first-order}{{6}{7}{Function with local maxima/minima (left), saddle point (middle) and global minimum (right)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Least Squares}{7}{subsection.3.3}}
\newlabel{eq-grad-ls}{{8}{8}{Least Squares}{equation.3.8}{}}
\newlabel{eq-sol-leastsquare}{{9}{8}{Least Squares}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Link with PCA}{8}{subsection.3.4}}
\newlabel{eq-pca-decomp}{{10}{8}{Link with PCA}{equation.3.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$. }}{9}{figure.7}}
\newlabel{fig-link-pca}{{7}{9}{Left: point clouds $(a_i)_i$ with associated PCA directions, right: quadratic part of $f(x)$}{figure.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Classification}{9}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Chain Rule}{9}{subsection.3.6}}
\newlabel{eq-grad-composition-linear}{{11}{9}{Chain Rule}{equation.3.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof. }}{10}{figure.8}}
\newlabel{fig-expansion-taylor}{{8}{10}{Left: First order Taylor expansion in 1-D and 2-D. Right: orthogonality of gradient and level sets and schematic of the proof}{figure.8}{}}
\newlabel{eq-differential-defn}{{12}{10}{Chain Rule}{equation.3.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Gradient Descent Algorithm}{10}{section.4}}
\newlabel{sec-grad-desc-basic}{{4}{10}{Gradient Descent Algorithm}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Steepest Descent Direction}{10}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Gradient Descent}{11}{subsection.4.2}}
\newlabel{eq-grad-desc}{{13}{11}{Gradient Descent}{equation.4.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right). }}{12}{figure.9}}
\newlabel{fig-gradesc}{{9}{12}{Influence of $\tau $ on the gradient descent (left) and optimal step size choice (right)}{figure.9}{}}
\newlabel{eq-armijo-rule}{{14}{12}{Armijo rule}{equation.4.14}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Convergence Analysis}{12}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Quadratic Case}{12}{subsection.5.1}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis for the quadratic case.}{12}{section*.3}}
\newlabel{prop-graddesc-quad}{{4}{12}{}{prop.4}{}}
\newlabel{eq-global-linrate-grad}{{15}{13}{}{equation.5.15}{}}
\newlabel{eq-best-rate-local}{{16}{13}{}{equation.5.16}{}}
\newlabel{eq-rate-strong-quad}{{17}{13}{Convergence analysis for the quadratic case}{equation.5.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  Contraction constant $h(\tau )$ for a quadratic function (right). }}{14}{figure.10}}
\newlabel{fig-grad-desc-contract}{{10}{14}{Contraction constant $h(\tau )$ for a quadratic function (right)}{figure.10}{}}
\newlabel{prop-graddesc-quad-sublin}{{5}{14}{}{prop.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}General Case}{15}{subsection.5.2}}
\@writefile{toc}{\contentsline {paragraph}{Hessian.}{15}{section*.4}}
\newlabel{eq-taylor-hess}{{18}{15}{Hessian}{equation.5.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Smoothness and strong convexity.}{16}{section*.5}}
\newlabel{eq-lipsch-grad}{{{$\mathcal  {R}_L$}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{eq-strong-conv}{{{$\mathcal  {S}_\mu $}}{16}{Smoothness and strong convexity}{equation.5.19}{}}
\newlabel{prop-smooth-strong}{{6}{16}{}{prop.6}{}}
\newlabel{eq-above-below-quad}{{19}{16}{}{equation.5.19}{}}
\newlabel{eq-upper-lower-bound-hess}{{20}{16}{}{equation.5.20}{}}
\@writefile{toc}{\contentsline {paragraph}{Convergence analysis.}{17}{section*.6}}
\newlabel{thm-gradsec-non-strong-conv}{{1}{17}{}{thm.1}{}}
\newlabel{eq-sublin-rate-gd}{{21}{17}{}{equation.5.21}{}}
\newlabel{eq-proox-x'rad-nonstrong-1}{{22}{17}{Convergence analysis}{equation.5.22}{}}
\newlabel{eq-conv-rate-proof-1}{{25}{18}{Convergence analysis}{equation.5.25}{}}
\newlabel{eq-rate-strong}{{26}{18}{Convergence analysis}{equation.5.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Acceleration}{18}{subsection.5.3}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Regularization}{19}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Penalized Least Squares}{19}{subsection.6.1}}
\newlabel{eq-regul-ls}{{27}{19}{Penalized Least Squares}{equation.6.27}{}}
\newlabel{eq-regul-constr}{{28}{19}{}{equation.6.28}{}}
\newlabel{eq-ineq-proof-regul}{{29}{20}{Penalized Least Squares}{equation.6.29}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Ridge Regression}{20}{subsection.6.2}}
\newlabel{eq-regul-ls-1}{{30}{20}{}{equation.6.30}{}}
\newlabel{eq-regul-ls-2}{{31}{20}{}{equation.6.31}{}}
\@writefile{toc}{\contentsline {paragraph}{Pseudo-inverse.}{20}{section*.7}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces $\ell ^q$ balls $ \left \{ x \tmspace  +\thickmuskip {.2777em};\tmspace  +\thickmuskip {.2777em} \DOTSB \sum@ \slimits@ _k |x_k|^q \leqslant 1 \right \} $ for varying $q$. }}{21}{figure.11}}
\newlabel{fig-sparsity-lq}{{11}{21}{$\ell ^q$ balls $\enscond {x}{\sum _k |x_k|^q \leq 1}$ for varying $q$}{figure.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Lasso}{21}{subsection.6.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Evolution with $\lambda $ of the function $F(x) \ensuremath  {\mathrel {\mathop {=}\limits ^{\unhbox \voidb@x \hbox {\upshape  \relax \fontsize  {5}{6}\selectfont  def.}}}}\frac  {1}{2}|\tmspace  -\thinmuskip {.1667em}| \cdot -y |\tmspace  -\thinmuskip {.1667em}|^2+\lambda |\cdot |$. }}{21}{figure.12}}
\newlabel{fig-varspars}{{12}{21}{Evolution with $\la $ of the function $F(x) \eqdef \frac {1}{2}\norm {\cdot -y}^2+\la |\cdot |$}{figure.12}{}}
\newlabel{prop-soft-tresdh}{{9}{21}{}{prop.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Iterative Soft Thresholding}{22}{subsection.6.4}}
\newlabel{sec-ista}{{6.4}{22}{Iterative Soft Thresholding}{subsection.6.4}{}}
\newlabel{eq-ista-surrog}{{32}{22}{Iterative Soft Thresholding}{equation.6.32}{}}
\newlabel{eq-ista}{{33}{22}{}{equation.6.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Stochastic Optimization}{23}{section.7}}
\newlabel{sec-stochastic-optim}{{7}{23}{Stochastic Optimization}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Minimizing Sums and Expectation}{23}{subsection.7.1}}
\newlabel{eq-min-sums}{{34}{23}{Minimizing Sums and Expectation}{equation.7.34}{}}
\newlabel{eq-min-int}{{35}{23}{Minimizing Sums and Expectation}{equation.7.35}{}}
\newlabel{eq-stochastic-erm}{{36}{23}{Minimizing Sums and Expectation}{equation.7.36}{}}
\newlabel{eq-stoch-logistic}{{37}{23}{Minimizing Sums and Expectation}{equation.7.37}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Batch Gradient Descent (BGD)}{23}{subsection.7.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces  Evolution of the error of the BGD for logistic classification. }}{24}{figure.13}}
\newlabel{fig-bgd}{{13}{24}{Evolution of the error of the BGD for logistic classification}{figure.13}{}}
\newlabel{eq-full-grad}{{38}{24}{Batch Gradient Descent (BGD)}{equation.7.38}{}}
\newlabel{eq-grad-formula}{{39}{24}{Batch Gradient Descent (BGD)}{equation.7.39}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Stochastic Gradient Descent (SGD)}{24}{subsection.7.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Unbiased gradient estimate}}{24}{figure.14}}
\newlabel{eq-unbiased-grad}{{40}{24}{Stochastic Gradient Descent (SGD)}{equation.7.40}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces  Display of a large number of trajectories $k \DOTSB \mapstochar \rightarrow x_k \in \mathbb  {R}$ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density. }}{25}{figure.16}}
\newlabel{fig-sgd-traject}{{16}{25}{Display of a large number of trajectories $k \mapsto x_k \in \RR $ generated by several runs of SGD. On the top row, each curve is a trajectory, and the bottom row displays the corresponding density}{figure.16}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Schematic view of SGD iterates}}{25}{figure.15}}
\newlabel{eq-stepsize-sgd}{{41}{25}{Stochastic Gradient Descent (SGD)}{equation.7.41}{}}
\newlabel{thm-conv-sgd}{{2}{25}{}{thm.2}{}}
\newlabel{eq-rate-sgd}{{42}{25}{}{equation.7.42}{}}
\newlabel{eq-sgd-proof-1}{{43}{25}{Stochastic Gradient Descent (SGD)}{equation.7.43}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces  Evolution of the error of the SGD for logistic classification (dashed line shows BGD). }}{26}{figure.17}}
\newlabel{fig-sgd}{{17}{26}{Evolution of the error of the SGD for logistic classification (dashed line shows BGD)}{figure.17}{}}
\newlabel{eq-sgd-proof-2}{{44}{26}{Stochastic Gradient Descent (SGD)}{equation.7.44}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.4}Stochastic Gradient Descent with Averaging (SGA)}{26}{subsection.7.4}}
\newlabel{sec-sga}{{7.4}{26}{Stochastic Gradient Descent with Averaging (SGA)}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.5}Stochastic Averaged Gradient Descent (SAG)}{27}{subsection.7.5}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Multi-Layers Perceptron}{27}{section.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces  Evolution of $\qopname  \relax o{log}_{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG. }}{28}{figure.18}}
\newlabel{fig-compariso-sgd}{{18}{28}{Evolution of $\log _{10}(f(x_k)-f(x^\star ))$ for SGD, SGA and SAG}{figure.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}MLP and its derivative}{28}{subsection.8.1}}
\@writefile{toc}{\contentsline {paragraph}{Expressiveness. }{28}{section*.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}MLP and Gradient Computation}{28}{subsection.8.2}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $u$.}{29}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Optimizing with respect to $W$.}{29}{section*.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Universality}{29}{subsection.8.3}}
\newlabel{sec-universality}{{8.3}{29}{Universality}{subsection.8.3}{}}
\newlabel{eq-constraint-univ}{{45}{29}{Universality}{equation.8.45}{}}
\newlabel{thm-universality}{{3}{29}{Cybenko, 1989}{thm.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Proof in dimension $p=1$.}{30}{section*.11}}
\@writefile{toc}{\contentsline {paragraph}{Proof in arbitrary dimension $p$.}{30}{section*.12}}
\newlabel{prop-proof-univ-1}{{11}{30}{}{prop.11}{}}
\newlabel{eq-prop-proof-univ-1}{{46}{30}{}{equation.8.46}{}}
\@writefile{toc}{\contentsline {paragraph}{Quantitative rates.}{31}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces  A computational graph. }}{32}{figure.19}}
\newlabel{fig-compgraph}{{19}{32}{A computational graph}{figure.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Automatic Differentiation}{32}{section.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Finite Differences and Symbolic Calculus}{32}{subsection.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Computational Graphs}{32}{subsection.9.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces  Relation between the variable for the forward (left) and backward (right) modes. }}{33}{figure.20}}
\newlabel{fig-forward-backward}{{20}{33}{Relation between the variable for the forward (left) and backward (right) modes}{figure.20}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Forward Mode of Automatic Differentiation}{33}{subsection.9.3}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{33}{section*.14}}
\newlabel{eq-simple-func-autodiff}{{47}{33}{Simple example}{equation.9.47}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces  Example of a simple computational graph. }}{34}{figure.21}}
\newlabel{fig-dag-example-simple}{{21}{34}{Example of a simple computational graph}{figure.21}{}}
\@writefile{toc}{\contentsline {paragraph}{Dual numbers.}{34}{section*.15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Reverse Mode of Automatic Differentiation}{35}{subsection.9.4}}
\@writefile{toc}{\contentsline {paragraph}{Back-propagation.}{35}{section*.16}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces  Complexity of forward (left) and backward (right) modes for composition of functions. }}{36}{figure.22}}
\newlabel{fig-matrix-mult}{{22}{36}{Complexity of forward (left) and backward (right) modes for composition of functions}{figure.22}{}}
\@writefile{toc}{\contentsline {paragraph}{Simple example.}{36}{section*.17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.5}Feed-forward Compositions}{36}{subsection.9.5}}
\newlabel{eq-simple-lin-dag}{{48}{36}{Feed-forward Compositions}{equation.9.48}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces  Computational graph for a feedforward architecture. }}{37}{figure.23}}
\newlabel{fig-mlp}{{23}{37}{Computational graph for a feedforward architecture}{figure.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.6}Feed-forward Architecture}{37}{subsection.9.6}}
\newlabel{eq-feednets}{{49}{37}{Feed-forward Architecture}{equation.9.49}{}}
\newlabel{eq-loss-feedf}{{50}{37}{Feed-forward Architecture}{equation.9.50}{}}
\newlabel{eq-backprop-discr}{{51}{37}{Feed-forward Architecture}{equation.9.51}{}}
\@writefile{toc}{\contentsline {paragraph}{Multilayers perceptron.}{37}{section*.18}}
\newlabel{eq-mlp-func}{{52}{37}{Multilayers perceptron}{equation.9.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces  Multi-layer perceptron parameterization. }}{38}{figure.24}}
\newlabel{fig-mlp-param}{{24}{38}{Multi-layer perceptron parameterization}{figure.24}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces  Computational graph for a recurrent architecture. }}{38}{figure.25}}
\newlabel{fig-recur}{{25}{38}{Computational graph for a recurrent architecture}{figure.25}{}}
\@writefile{toc}{\contentsline {paragraph}{Link with adjoint state method.}{38}{section*.19}}
\newlabel{eq-flow-eq}{{53}{38}{Link with adjoint state method}{equation.9.53}{}}
\newlabel{eq-ode-structure}{{54}{38}{Link with adjoint state method}{equation.9.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.7}Recurrent Architectures}{38}{subsection.9.7}}
\newlabel{eq-feednets-recur}{{55}{38}{Recurrent Architectures}{equation.9.55}{}}
\newlabel{eq-backprop-discr}{{56}{38}{Recurrent Architectures}{equation.9.56}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces  Recurrent residual perceptron parameterization. }}{39}{figure.26}}
\newlabel{fig-recurrent-param}{{26}{39}{Recurrent residual perceptron parameterization}{figure.26}{}}
\newlabel{eq-jacobian-mlp}{{57}{39}{Recurrent Architectures}{equation.9.57}{}}
\@writefile{toc}{\contentsline {paragraph}{Residual recurrent networks. }{39}{section*.20}}
\@writefile{toc}{\contentsline {paragraph}{Mitigating memory requirement. }{39}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Fixed point maps}{40}{section*.22}}
\newlabel{eq-impl-func-formula}{{58}{40}{Fixed point maps}{equation.9.58}{}}
\@writefile{toc}{\contentsline {paragraph}{Argmin layers}{40}{section*.23}}
\newlabel{eq-argmin-layer}{{59}{40}{Argmin layers}{equation.9.59}{}}
\newlabel{eq-danskin}{{60}{40}{Argmin layers}{equation.9.60}{}}
\@writefile{toc}{\contentsline {paragraph}{Sinkhorn's algorithm}{40}{section*.24}}
\bibstyle{plain}
\bibdata{all}
